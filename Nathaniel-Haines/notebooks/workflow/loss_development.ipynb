{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: All models and code used here were developed purely for the purpose of illustration. We use a different set of models in production, although the principles are all the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import dill\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import arviz as az\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from cmdstanpy import CmdStanModel\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using is publically available [here](https://www.casact.org/publications-research/research/research-resources/loss-reserving-data-pulled-naic-schedule-p). Specifically, we are using the \"PP Auto Data Set\" (direct download [here](https://www.casact.org/sites/default/files/2021-04/ppauto_pos.csv)). These data include information on private passenger auto policy premiums and losses. The specifics are not too important for our purposes. The important note is that we have data from a number of different auto programs, which makes hierarchical models a natural means to tackle the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"..\", \"..\", \"data\", \"ppauto_pos.csv\")\n",
    "MODEL_PATH = os.path.join(\"..\", \"..\", \"stan\", \"chain_ladder_hierarchical.stan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are just doing some preprocessing on the raw data to compute some quantities that we will use later, as well as to remove certain insurance programs from the dataset that have strange values. In reality, we would normally want to be more careful about this step. But for illustration, we are being rather cavelier with removing programs that are \"extreme\" in some sense. At the end, we are plotting the distributions of \"loss ratios\" in the data, which is the quantity we will end up modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_auto = pd.read_csv(DATA_PATH)\n",
    "\n",
    "pp_auto[\"IncurLoss_B\"][pp_auto[\"IncurLoss_B\"]<0] = np.nan\n",
    "pp_auto[\"EarnedPremDIR_B\"][pp_auto[\"EarnedPremDIR_B\"]<0] = np.nan\n",
    "pp_auto[\"loss_ratio\"] = (pp_auto[\"IncurLoss_B\"] / pp_auto[\"EarnedPremDIR_B\"]).replace([np.inf, -np.inf], np.nan)\n",
    "pp_auto[\"loss_ratio\"][pp_auto[\"loss_ratio\"]<=0.0] = np.nan\n",
    "pp_auto[\"loss_ratio\"][pp_auto[\"loss_ratio\"]>5] = np.nan\n",
    "pp_auto = pp_auto.dropna(axis=0)\n",
    "\n",
    "px.histogram(x=pp_auto[\"loss_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below just grabs data from an individual program (indicated by `code`) and gives us a few different quantities used in modeling. We are also creating a `loss_obs` variable that we will use for training models, which makes it easy to later look at out-of-sample performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_data(df, code):\n",
    "    df = df.loc[df[\"GRCODE\"] == code]\n",
    "    loss_true = np.array(pd.pivot(df, index=\"AccidentYear\", columns=\"DevelopmentLag\", values=\"loss_ratio\"))\n",
    "    premium = np.array(pd.pivot(df, index=\"AccidentYear\", columns=\"DevelopmentLag\", values=\"EarnedPremDIR_B\"))\n",
    "    AY, DL = loss_true.shape\n",
    "    loss_obs = copy.copy(loss_true)\n",
    "    \n",
    "    for i in range(AY):\n",
    "        for j in range(DL):\n",
    "            loss_obs[i,j] = loss_obs[i,j] if i+j < 10 else -1 \n",
    "\n",
    "    return loss_obs, loss_true, premium, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "premium_data = []\n",
    "code_data = []\n",
    "\n",
    "for program_code in pp_auto[\"GRCODE\"].unique()[:30]:\n",
    "    loss_train, loss_test, premium, code = get_loss_data(pp_auto, program_code)\n",
    "    if loss_test.shape == loss_train.shape == (10,10):\n",
    "        if not np.isnan(loss_train).any() and not (loss_train == 0).any():\n",
    "            train_data.append(loss_train)\n",
    "            test_data.append(loss_test)\n",
    "            premium_data.append(premium)\n",
    "            code_data.append(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating through the first 30 programs and including them in our pre-processed data if they meet the criteria specified in the code above, we are left with 25 programs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(code_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we put everything into a dictionary for Stan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data = {\n",
    "    \"N\": len(train_data),\n",
    "    \"AY\": train_data[0].shape[0],\n",
    "    \"DL\": train_data[0].shape[1],\n",
    "    \"loss\": train_data,\n",
    "    \"prior_only\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chain-Ladder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chain-Ladder model below is a simplified variant of the type of model we use at Ledger Investing to \"develop\" losses to their \"ultimate\" state. The idea is that, for any given year in which accidents occur, it can take years for us to know what the final (or ultimate) loss ratio (ratio of losses paid out vs premium coming in) will be for the given year (e.g., court cases, adjustments, etc. can take a long time). \n",
    "\n",
    "The model below assumes that loss ratios in subsequent \"development lags\" (i.e. years since looking back at the original accident year) are a function of a free parameter $\\text{ATA}_d$ and the previous year's view we had of the loss ratio. \n",
    "\n",
    "Then, the the variance is expected to decrease for our view of a given loss ratio as we look at that year further into the future. \n",
    "\n",
    "Finally, we assume that the loss ratio for the next development lag will be gamma distributed. The parameter transformations just take our mean+standard deviation parameterization and re-parameterize to the shape+inverse scale parameters of the gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{LR}_{t,d} &\\sim \\Gamma(\\alpha_{t,d}, \\beta_{t,d}) \\\\\n",
    "    \\alpha_{t,d} &= \\mu_{t,d}^2 / \\sigma_{t,d}^2 \\\\\n",
    "    \\beta_{t,d} &= \\mu_{t,d} / \\sigma_{t,d}^2 \\\\\n",
    "    \\mu_{t,d} &= \\text{ATA}_d \\cdot \\text{LR}_{t,d-1} \\\\\n",
    "    \\sigma_{t,d} &= \\exp(\\sigma_\\text{int} + \\sigma_\\text{slope} \\cdot [d-1])\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{LR}_{t,d} &\\sim \\Gamma(\\mu_{t,d}^2 / \\sigma_{t,d}^2, \\mu_{t,d} / \\sigma_{t,d}^2) \\\\\n",
    "    \\mu_{t,d} &= \\text{ATA}_d \\cdot \\text{LR}_{t,d-1} \\\\\n",
    "    \\sigma_{t,d} &= \\exp(\\sigma_\\text{int} + \\sigma_\\text{slope} \\cdot [d-1])\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile and sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_ladder = CmdStanModel(\n",
    "    \"chain_ladder_hierarchical\",\n",
    "    stan_file=MODEL_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data[\"prior_only\"] = 0\n",
    "fit = chain_ladder.sample(\n",
    "    stan_data,\n",
    "    iter_warmup=1000, \n",
    "    iter_sampling=1000,\n",
    "    inits=0.2,\n",
    "    chains=4,\n",
    "    parallel_chains=4,\n",
    "    adapt_delta=.8,\n",
    "    seed=43215,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some wanrings having to do with exceptions that occur due to the parameter transformations we are making, but overall diagnostics look pretty good: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the posteriors in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(\n",
    "    az.from_cmdstanpy(fit),\n",
    "    var_names=['.*groupmean', '.*groupSD'],\n",
    "    filter_vars=\"regex\",\n",
    "    kind='scatter',\n",
    "    divergences=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(\n",
    "    az.from_cmdstanpy(fit),\n",
    "    var_names=['sigma_int', 'sigma_slope'],\n",
    "    kind='scatter',\n",
    "    divergences=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(\n",
    "    az.from_cmdstanpy(fit),\n",
    "    var_names=['mu_ata_program', 'sigma_ata_program'],\n",
    "    kind='scatter',\n",
    "    divergences=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below makes it easy to plot out the posterior predictions against the observed and test data for each accident year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(pred_loss, test_data, program_idx):\n",
    "    fig = make_subplots(2,5)\n",
    "\n",
    "    x = np.arange(10)\n",
    "    data_mu = np.nanmean(pred_loss[:,program_idx,:,:], axis=0)\n",
    "    data_lower = np.nanquantile(pred_loss[:,program_idx,:,:], 0.05, axis=0)\n",
    "    data_upper = np.nanquantile(pred_loss[:,program_idx,:,:], 0.95, axis=0)\n",
    "\n",
    "    colors = px.colors.sequential.Viridis\n",
    "\n",
    "    for i in range(10):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x, \n",
    "                y=data_mu[i], \n",
    "                mode=\"lines\",\n",
    "                marker=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ), \n",
    "            1 if i<5 else 2, (i % 5) + 1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=data_upper[i],\n",
    "                mode='lines',\n",
    "                marker=dict(color=colors[i]),\n",
    "                line=dict(width=0),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            1 if i<5 else 2, (i % 5) + 1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=data_lower[i],\n",
    "                mode='lines',\n",
    "                marker=dict(color=colors[i]),\n",
    "                line=dict(width=0),\n",
    "                fill='tonexty',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            1 if i<5 else 2, (i % 5) + 1\n",
    "        )\n",
    "        \n",
    "    for i in range(10):\n",
    "        row = test_data[program_idx][i]\n",
    "        row[row==-1] = np.nan\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x, \n",
    "                y=row,\n",
    "                mode='markers',\n",
    "                name=None,\n",
    "                showlegend=False,\n",
    "                marker=dict(color=colors[i], size=10),\n",
    "            ),\n",
    "            1 if i<5 else 2, (i % 5) + 1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x, \n",
    "                y=row,\n",
    "                mode='markers',\n",
    "                name=None,\n",
    "                showlegend=False,\n",
    "                marker=dict(\n",
    "                    color=colors[i] if i==0 else [0 if train_data[program_idx][i][j] != -1 else 1 for j in range(len(row))], \n",
    "                    colorscale=[[0, colors[i]], [1, \"white\"]]\n",
    "                ),\n",
    "            ),\n",
    "            1 if i<5 else 2, (i % 5) + 1\n",
    "        )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the posterior predictions for the program where `index=1`, but you can change the program index to check out performance for different programs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loss = fit.stan_variable(var=\"pred_loss\")\n",
    "plot_predictions(pred_loss, test_data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will save out the predictions and other information to use in our froecasting model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", \"..\", \"data\", \"developed_losses.pkl\"), \"wb\") as outfile: \n",
    "    dill.dump(pred_loss, outfile)\n",
    "\n",
    "with open(os.path.join(\"..\", \"..\", \"data\", \"premium_data.pkl\"), \"wb\") as outfile: \n",
    "    dill.dump(premium_data, outfile)\n",
    "\n",
    "with open(os.path.join(\"..\", \"..\", \"data\", \"loss_dev_test_data.pkl\"), \"wb\") as outfile: \n",
    "    dill.dump(np.array(test_data), outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stancon-2023",
   "language": "python",
   "name": "stancon-2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
